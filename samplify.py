#!/usr/bin/env python
# coding: utf-8
# Application samplify.py (Segmentation and Classification tool for images of Arabidopsis Seeds)
# Author: Ronja Lea Jennifer MÃ¼ller, Potsdam 2025

# In[1]:


# Copyright (c) Meta Platforms, Inc. and affiliates.


# # Automatically generating object masks with SAM

# Since SAM can efficiently process prompts, masks for the entire image can be generated by sampling a large number of prompts over an image. This method was used to generate the dataset SA-1B.
# 
# The class `SamAutomaticMaskGenerator` implements this capability. It works by sampling single-point input prompts in a grid over the image, from each of which SAM can predict multiple masks. Then, masks are filtered for quality and deduplicated using non-maximal suppression. Additional options allow for further improvement of mask quality and quantity, such as running prediction on multiple crops of the image or postprocessing masks to remove small disconnected regions and holes.

# ## Set-up

# In[2]:


import numpy as np
import matplotlib.pyplot as plt
import cv2
import torch
import os
import sys
import time
import traceback
import joblib
import pandas as pd
from glob import glob
from tqdm import tqdm  # Progress bar
import argparse
import subprocess
import math
import random
import openpyxl
from skimage.feature import local_binary_pattern, graycomatrix, graycoprops
import torchvision.transforms.functional as TF
from scipy.fftpack import fft
import concurrent.futures
from concurrent.futures import ThreadPoolExecutor
import warnings
from openpyxl import load_workbook
from openpyxl.utils.dataframe import dataframe_to_rows
from datetime import datetime
warnings.filterwarnings("ignore", category=UserWarning, module="sklearn.utils.validation")


# In[3]:

def start_env():
    #select the device for computation
    if torch.cuda.is_available():
        device = torch.device("cuda")
    elif torch.backends.mps.is_available():
        device = torch.device("mps")
    else:
        device = torch.device("cpu")
    print(f"using device: {device}")
    
    if device.type == "cuda":
        # use bfloat16 for the entire notebook
        torch.autocast("cuda", dtype=torch.bfloat16).__enter__()
        # turn on tfloat32 for Ampere GPUs (https://pytorch.org/docs/stable/notes/cuda.html#tensorfloat-32-tf32-on-ampere-devices)
        if torch.cuda.get_device_properties(0).major >= 8:
            torch.backends.cuda.matmul.allow_tf32 = True
            torch.backends.cudnn.allow_tf32 = True
    elif device.type == "mps":
        print(
            "\nSupport for MPS devices is preliminary. SAM 2 is trained with CUDA and might "
            "give numerically different outputs and sometimes degraded performance on MPS. "
            "See e.g. https://github.com/pytorch/pytorch/issues/84936 for a discussion."
        )
    
    
    # In[4]:
    
    
    from sam2.build_sam import build_sam2
    from sam2.automatic_mask_generator import SAM2AutomaticMaskGenerator
    sam2_checkpoint = "../checkpoints/sam2.1_hiera_large.pt"
    model_cfg = "configs/sam2.1/sam2.1_hiera_l.yaml"
    
    sam2 = build_sam2(model_cfg, sam2_checkpoint, device=device, apply_postprocessing=False)
    
    
    # In[5]:
    
    
    mask_generator = SAM2AutomaticMaskGenerator(
        model=sam2,
        points_per_side=49,
        pred_iou_thresh=0.86,
        stability_score_thresh=0.92,
        crop_n_layers=1,
        crop_n_points_downscale_factor=1
    )
    return mask_generator


# ## functions

# In[6]:


# Resize the image while maintaining aspect ratio
def resize_image(image, max_dim=1024):
    """Resizes the image to have the largest dimension equal to max_dim while maintaining aspect ratio."""
    height, width = image.shape[:2]
    if max(height, width) > max_dim:
        if height > width:
            new_height = max_dim
            new_width = int(width * (max_dim / height))
        else:
            new_width = max_dim
            new_height = int(height * (max_dim / width))
        image = cv2.resize(image, (new_width, new_height))
    return image


# In[7]:


def contours_sam(original_dense_image, resized_image, masks, dense_mask, shift_of_dense_region, original_big_image):
    original_height, original_width = original_dense_image.shape[:2]  # Store original dimensions
    resized_height, resized_width = resized_image.shape[:2]
    dense_mask_inv = 1 - dense_mask
    x, y = shift_of_dense_region

    all_contours = []

    for mask_data in masks:
        mask = mask_data["segmentation"]  # Get the segmentation mask

        # Resize the mask to the original image dimensions
        resized_mask = cv2.resize(mask.astype(np.uint8), (original_width, original_height), interpolation=cv2.INTER_LINEAR)
        resized_mask = resized_mask.astype(bool) # Convert back to boolean

        contours, _ = cv2.findContours(resized_mask.astype(np.uint8), cv2.RETR_EXTERNAL, cv2.CHAIN_APPROX_SIMPLE)
        
        if contours:
            adjusted_contours = [contour + [x, y] for contour in contours]
            all_contours.extend(adjusted_contours)

    return all_contours


# In[8]:


def find_dense_region(image):
    # Step 1: Convert to grayscale and apply Gaussian blur
    gray = cv2.cvtColor(image, cv2.COLOR_BGR2GRAY)
    blurred = cv2.GaussianBlur(gray, (15, 15), 0)
    
    # Step 2: Apply thresholding to create a binary mask
    _, binary = cv2.threshold(blurred, 0, 255, cv2.THRESH_BINARY_INV + cv2.THRESH_OTSU)

     # Step 3: Remove unwanted thin edge contours touching the image border
    contours, _ = cv2.findContours(binary, cv2.RETR_EXTERNAL, cv2.CHAIN_APPROX_SIMPLE)
    for cnt in contours:
        x, y, w, h = cv2.boundingRect(cnt)
        if x == 0 or y == 0 or x + w == image.shape[1] or y + h == image.shape[0]:
            cv2.drawContours(blurred, [cnt], -1, (255,255,255), thickness=cv2.FILLED)
            cv2.drawContours(image, [cnt], -1, (255,255,255), thickness=cv2.FILLED) #mask the boundary contours to not mess up thresh in sparse region seed detection later

    # Apply thresholding to create a binary mask again (because no black edge boundaries that give false thresh results and forget light seeds
    _, binary = cv2.threshold(blurred, 0, 255, cv2.THRESH_BINARY_INV + cv2.THRESH_OTSU)

    # Step 4: Dilate the binary mask to emphasize dense regions
    kernel = np.ones((10, 10), np.uint8)
    binary = cv2.dilate(binary, kernel, iterations=30)

    # Step 5: Identify the largest contour as the dense region
    contours, _ = cv2.findContours(binary, cv2.RETR_EXTERNAL, cv2.CHAIN_APPROX_SIMPLE)
    contour_areas = [cv2.contourArea(cnt) for cnt in contours]
    largest_contour = contours[np.argmax(contour_areas)]

    # Step 6: Create a mask for the dense region
    dense_mask = np.zeros_like(gray, dtype=np.uint8)
    cv2.drawContours(dense_mask, [largest_contour], -1, 255, thickness=cv2.FILLED)

    # Step 7: Extract the dense region
    x, y, w, h = cv2.boundingRect(largest_contour)
    dense_region_full = cv2.bitwise_and(image, image, mask=dense_mask)
    dense_region = dense_region_full[y:y+h, x:x+w]
    dense_region[np.all(dense_region == [0, 0, 0], axis=-1)] = [255, 255, 255]  
    shift_of_dense_region = (x, y)

    # Step 8: Create the sparse region by inverting the dense mask
    median_color = np.median(image.reshape(-1, 3), axis=0).astype(int)
    sparse_mask = cv2.bitwise_not(dense_mask)
    sparse_region = cv2.bitwise_and(image, image, mask=sparse_mask)
    sparse_region[np.all(sparse_region == [0, 0, 0], axis=-1)] = median_color.tolist()

    return dense_region, sparse_region, dense_mask, shift_of_dense_region


# ## Process the contours

# In[10]:


def remove_duplicate_bounding_boxes_with_tolerance(feat_list, tolerance=None, min_overlap=0.45):
    """Remove entries from the feat_list that have bounding boxes with centroids within the tolerance
    and bounding box areas that are at least 45% similar."""
    unique_feat_list = []

    for features in feat_list:
        current_centroid = features["Centroid"]
        current_bbox = features["Bounding Box"]
        current_area = bounding_box_area(current_bbox)
        is_duplicate = False

        # Compare the current centroid and bounding box area with those of previous entries
        for unique_features in unique_feat_list:
            unique_centroid = unique_features["Centroid"]
            unique_bbox = unique_features["Bounding Box"]
            unique_area = bounding_box_area(unique_bbox)

            # If the centroids are close enough and bounding box areas are similar, it's a duplicate
            if (euclidean_distance(current_centroid, unique_centroid) <= tolerance and
                are_areas_similar(current_area, unique_area, min_overlap)):
                is_duplicate = True
                break

        if not is_duplicate:
            unique_feat_list.append(features)

    return unique_feat_list

def bounding_box_area(bbox):
    """Calculate the area of a bounding box given as (x, y, w, h)."""
    _, _, w, h = bbox
    return w * h

def euclidean_distance(point1, point2):
    """Calculate the Euclidean distance between two points."""
    return np.sqrt((point1[0] - point2[0]) ** 2 + (point1[1] - point2[1]) ** 2)

def are_areas_similar(area1, area2, min_overlap=0.8):
    """Check if the two areas have at least 80% overlap."""
    smaller_area = min(area1, area2)
    larger_area = max(area1, area2)

    return smaller_area / larger_area >= min_overlap


# In[11]:


def get_features_short(contours, original_img, mean_background_color):
    """
    Computes basic features: Area, Perimeter, Hull Perimeter, Mean Gray, and Parameter.
    """
    features_list = []
    h, w, _ = original_img.shape
    gray = cv2.cvtColor(original_img, cv2.COLOR_BGR2GRAY)

    mean_background_gray = np.mean(gray)

    for i, contour in enumerate(contours):
        # Compute basic contour properties
        area = cv2.contourArea(contour)
        perimeter = cv2.arcLength(contour, True)
        hull = cv2.convexHull(contour)
        hull_perimeter = cv2.arcLength(hull, True)

        # Compute normalized mean gray value
        mask = np.zeros_like(gray)
        cv2.drawContours(mask, [contour], -1, 255, -1)
        mean_gray = np.mean(gray[mask == 255]) / mean_background_gray

        # Additional Parameter
        parameter = perimeter / np.sqrt(area) if area > 0 else 0

        # Save the basic features for this contour
        features_list.append({
            "Area": area,
            "Perimeter": perimeter,
            "Hull Perimeter": hull_perimeter,
            "Mean Gray": mean_gray,
            "Parameter": parameter,
            "Contour": contour,  # Keep contour for further calculations
        })
    
    return features_list, mean_background_color

def process_extended_features(feature, original_img_tensor, gray_tensor, mean_background_color):
    contour = feature["Contour"]
    bbox = cv2.boundingRect(contour)
    contour_np = np.array(contour)

    # Step 1: Convex Hull, Area, and Solidity
    hull = cv2.convexHull(contour_np)
    hull_area = cv2.contourArea(hull)
    M = cv2.moments(contour_np)
    cx, cy = (int(M["m10"] / M["m00"]), int(M["m01"] / M["m00"])) if M["m00"] else (0, 0)
    solidity = feature["Area"] / hull_area if hull_area > 0 else 0

    # Step 2: Mask creation and color computations
    mask = np.zeros((original_img_tensor.shape[1], original_img_tensor.shape[2]), dtype=np.uint8)
    cv2.drawContours(mask, [contour_np], -1, 255, -1)
    mask_tensor = torch.tensor(mask, device='cuda')

    # Ensure mask dimensions align with the image
    if mask_tensor.shape != original_img_tensor.shape[1:]:
        raise ValueError(f"Mask shape {mask_tensor.shape} does not match image shape {original_img_tensor.shape[1:]}")

    # Extract masked regions for color computations
    masked_pixels = original_img_tensor[:, mask_tensor == 255]

    # Convert to float32 before performing operations
    masked_pixels = masked_pixels.float()

    # Handle empty masks gracefully
    if masked_pixels.numel() == 0:
        mean_color_rgb = np.array([0, 0, 0])
        min_color_rgb = np.array([0, 0, 0])
        max_color_rgb = np.array([0, 0, 0])
    else:
        mean_color_rgb = masked_pixels.mean(dim=1).cpu().numpy()
        min_color_rgb = masked_pixels.min(dim=1).values.cpu().numpy()
        max_color_rgb = masked_pixels.max(dim=1).values.cpu().numpy()

    # Normalize color values by background color
    mean_red, mean_green, mean_blue = np.clip(mean_color_rgb / mean_background_color, 0, None)
    min_red, min_green, min_blue = np.clip(min_color_rgb / mean_background_color, 0, None)
    max_red, max_green, max_blue = np.clip(max_color_rgb / mean_background_color, 0, None)

    # Step 3: Color Moments (Mean, Variance, Skewness) for each channel
    def compute_color_moments(channel_data):
        mean_val = torch.mean(channel_data).item()
        variance_val = torch.var(channel_data).item()
        skewness_val = torch.mean((channel_data - mean_val) ** 3).item()
        return mean_val, variance_val, skewness_val

    if masked_pixels.numel() == 0:
        mean_b = mean_g = mean_r = var_b = var_g = var_r = skew_b = skew_g = skew_r = 0
    else:
        hist_r, hist_g, hist_b = masked_pixels[0].float(), masked_pixels[1].float(), masked_pixels[2].float()
        mean_b, var_b, skew_b = compute_color_moments(hist_b)
        mean_g, var_g, skew_g = compute_color_moments(hist_g)
        mean_r, var_r, skew_r = compute_color_moments(hist_r)

    # Step 4: Fourier Descriptors
    try:
        complex_boundary = torch.tensor([complex(pt[0] - cx, pt[1] - cy) for pt in contour_np[:, 0, :]], device='cuda')
        fourier_descriptors = torch.abs(torch.fft.fft(complex_boundary))[:10]
        fourier_energy = fourier_descriptors.sum().item()
    except Exception:
        fourier_energy = 0

    # Step 5: Haralick Texture Features (Still using CPU for this part)
    gray_cpu = gray_tensor.cpu().numpy()  # Move to CPU for Haralick computation
    glcm = graycomatrix(gray_cpu, distances=[1], angles=[0], symmetric=True, normed=True)
    contrast = graycoprops(glcm, 'contrast')[0, 0]
    energy = graycoprops(glcm, 'energy')[0, 0]
    homogeneity = graycoprops(glcm, 'homogeneity')[0, 0]
    correlation = graycoprops(glcm, 'correlation')[0, 0]

    # Step 6: Circularity
    perimeter = feature["Perimeter"]
    circularity = (4 * np.pi * feature["Area"]) / (perimeter ** 2) if perimeter > 0 else 0

    # Step 7: Eccentricity and Overlap Ratio (New)
    try:
        ellipse = cv2.fitEllipse(contour_np)
        major_axis, minor_axis = max(ellipse[1]), min(ellipse[1])
        eccentricity = np.sqrt(1 - (minor_axis ** 2) / (major_axis ** 2))

        # Create a tensor of zeros with the same shape as gray_tensor
        ellipse_mask = torch.zeros_like(gray_tensor, dtype=torch.uint8)
        ellipse_mask = ellipse_mask.cpu().numpy()
        cv2.ellipse(ellipse_mask, ellipse, 255, thickness=-1)
        intersection = np.logical_and(ellipse_mask, mask).sum()
        union = np.logical_or(ellipse_mask, mask).sum()
        overlap_ratio = intersection / union if union else 0
    except cv2.error:
        eccentricity, overlap_ratio = 0, 0

    # Step 8: Pixel Intensity Along Contour (Contour-based Feature)
    def get_intensity_along_contour(image_tensor, contour):
        # Convert the image tensor to a 2D array for easier access of pixel values
        intensities = []
        for point in contour:
            x, y = point[0]
            pixel_value = image_tensor[:, y, x]  # Access the RGB channels directly
            intensities.append(pixel_value.cpu().numpy())  # Convert to NumPy for further computation
        return np.array(intensities)

    contour_intensities = get_intensity_along_contour(original_img_tensor, contour)
    mean_intensity = np.mean(contour_intensities)
    var_intensity = np.var(contour_intensities)

    # Update feature dictionary with all computed features
    feature.update({
        "Bounding Box": bbox,
        "Centroid": (cx, cy),
        "Solidity": solidity,
        "Eccentricity": eccentricity,
        "Overlap Ratio": overlap_ratio,
        "Circularity": circularity,
        "Fourier Energy": fourier_energy,
        "Haralick Contrast": contrast,
        "Haralick Energy": energy,
        "Haralick Homogeneity": homogeneity,
        "Haralick Correlation": correlation,
        # Mean Colors
        "Mean Red": mean_red,
        "Mean Green": mean_green,
        "Mean Blue": mean_blue,
        # Min Colors
        "Min Red": min_red,
        "Min Green": min_green,
        "Min Blue": min_blue,
        # Max Colors
        "Max Red": max_red,
        "Max Green": max_green,
        "Max Blue": max_blue,
        # Color Moments
        "Color Moment Mean Red": mean_r,
        "Color Moment Mean Green": mean_g,
        "Color Moment Mean Blue": mean_b,
        "Color Moment Variance Red": var_r,
        "Color Moment Variance Green": var_g,
        "Color Moment Variance Blue": var_b,
        "Color Moment Skewness Red": skew_r,
        "Color Moment Skewness Green": skew_g,
        "Color Moment Skewness Blue": skew_b,
        # Pixel Intensity
        "Mean Intensity Along Contour": mean_intensity,
        "Variance Intensity Along Contour": var_intensity,
    })

    torch.cuda.empty_cache()
    return feature

def extend_features(features_list, original_img, mean_background_color, batch_size=4):
    """
    Processes features in parallel while managing GPU memory efficiently.
    If out of memory, reduces batch size; if that fails, switches to non-parallel execution.
    If still failing, skips the image.
    """
    try:
        # Convert image to tensor and move to GPU once
        original_img_tensor = torch.tensor(original_img).permute(2, 0, 1).to('cuda', non_blocking=True)
        gray_tensor = TF.rgb_to_grayscale(original_img_tensor.float() / 255).squeeze(0).mul(255).byte()

        extended_features_list = []
        batch_size = min(batch_size, max(1, len(features_list) // 10))  # Adjust batch size dynamically

        for i in range(0, len(features_list), batch_size):
            batch = features_list[i:i + batch_size]

            try:
                with ThreadPoolExecutor(max_workers=min(4, batch_size)) as executor:
                    futures = [executor.submit(process_extended_features, feature, original_img_tensor, gray_tensor, mean_background_color)
                               for feature in batch]
                    for future in futures:
                        extended_features_list.append(future.result())

                # Free GPU memory after processing each batch
                del batch
                torch.cuda.empty_cache()

            except RuntimeError as e:
                if "CUDA out of memory" in str(e):
                    print("Warning: CUDA out of memory. Retrying with reduced batch size...")
                    torch.cuda.empty_cache()
                    return extend_features(features_list, original_img, mean_background_color, batch_size=max(1, batch_size // 2))

        return extended_features_list

    except RuntimeError as e:
        if "CUDA out of memory" in str(e):
            print("Warning: Severe CUDA OOM. Switching to non-parallel execution...")
            torch.cuda.empty_cache()
            return extend_features_single_thread(features_list, original_img, mean_background_color)

        print(f"Skipping image due to unexpected error: {e}")
        return []  # Skip this image if it still fails

def extend_features_single_thread(features_list, original_img, mean_background_color):
    """
    Fallback: Processes features one by one without parallelization if parallel execution fails.
    """
    try:
        original_img_tensor = torch.tensor(original_img).permute(2, 0, 1).to('cuda', non_blocking=True)
        gray_tensor = TF.rgb_to_grayscale(original_img_tensor.float() / 255).squeeze(0).mul(255).byte()

        extended_features_list = []
        for feature in features_list:
            try:
                extended_features_list.append(process_extended_features(feature, original_img_tensor, gray_tensor, mean_background_color))
            except RuntimeError as e:
                if "CUDA out of memory" in str(e):
                    print("Warning: CUDA OOM in single-threaded mode. Skipping this image.")
                    torch.cuda.empty_cache()
                    return []  # Skip image if single-threaded mode still fails

        return extended_features_list

    except RuntimeError as e:
        print(f"Skipping image due to critical error: {e}")
        return []  # Skip this image completely

# In[12]:


def extract_seeds(image, feature_list, median_perimeter = None):
    seed_features = []
    
    if median_perimeter is None:
        perimeters = [feature["Hull Perimeter"] for feature in feature_list if "Hull Perimeter" in feature]
        median_perimeter = np.median(perimeters)

    for features in feature_list:
        hull_perimeter = features["Hull Perimeter"]
        color = features["Mean Gray"]
        parameter = features["Parameter"]

        # Exclude very light regions (e.g., white artifacts due to region/SAM computations)
        if color > 0.9: 
            continue

        # Apply perimeter and parameter filters
        if hull_perimeter < 0.5 * median_perimeter:
            continue
        elif hull_perimeter <= 1.55 * median_perimeter and parameter < 5.8:
            seed_features.append(features)


    if len(seed_features) < 10:
        median_perimeter = None

    return seed_features, median_perimeter


# ## Labelings

# In[13]:


def map_labels_to_features(features_list, labeled_points):
    """
    Maps label information from labeled points to features based on the closest label to the centroid
    if multiple labels are within the bounding box. Each label can only be assigned to one feature.

    :param features_list: List of feature dictionaries, each with properties including bounding box and centroid.
    :param labeled_points: List of tuples [(x, y, label), ...].
    :return: Updated features_list with labels mapped.
    """
    # Keep track of used labeled points
    used_points = set()

    for feature in features_list:
        bbox_x, bbox_y, bbox_w, bbox_h = feature["Bounding Box"]
        centroid_x, centroid_y = feature["Centroid"]

        closest_label = None
        min_distance = float('inf')

        for idx, (point_x, point_y, label) in enumerate(labeled_points):
            if idx in used_points:
                continue

            if bbox_x <= point_x <= bbox_x + bbox_w and bbox_y <= point_y <= bbox_y + bbox_h:
                # Calculate squared distance to avoid unnecessary sqrt
                dist_sq = (centroid_x - point_x)**2 + (centroid_y - point_y)**2

                if dist_sq < min_distance:
                    min_distance = dist_sq
                    closest_label = (label, idx)

        if closest_label:
            # Extract the label from the closest_label tuple
            label, idx = closest_label  # Assuming the second element is the label (e.g., 0, 1, 2)

            if label not in [0, 1, 2]:
                raise ValueError(f"Invalid label {label} encountered. Only labels 0, 1, or 2 are allowed.")
                
            if closest_label:
                label, idx = closest_label
                if label == 0:
                    feature["Label"] = "Normal"
                elif label == 1:
                    feature["Label"] = "Partially"
                elif label == 2:
                    feature["Label"] = "Aborted"
                else:
                    raise ValueError(f"Invalid label {label} encountered. Only labels 0, 1, or 2 are allowed.")

            # Mark the corresponding labeled point as used
            used_points.add(idx)  # Assuming the third element is the unique identifier
            
        else:
            feature["Label"] = None  # No label found for this feature


    return features_list



def read_coordinates_from_txt(file_path, x_col='X', y_col='Y', label_col='Counter', delimiter='\t'):
    """
    Read coordinates and labels from a TXT file.

    :param file_path: Path to the TXT file containing coordinates and labels.
    :param x_col: Column name for X coordinates.
    :param y_col: Column name for Y coordinates.
    :param label_col: Column name for label (e.g., 'Counter' or 'Count').
    :param delimiter: Delimiter used in the TXT file (default is tab).
    :return: List of tuples (x, y, label) or 0 if x_col is missing.
    """
    try:
        df = pd.read_csv(file_path, delimiter=delimiter)

        if x_col not in df.columns or y_col not in df.columns or label_col not in df.columns:
            raise ValueError(f"File is missing required columns: {x_col}, {y_col}, or {label_col}.")

        return list(zip(df[x_col], df[y_col], df[label_col]))
    except Exception as e:
        print(f"Error reading file {file_path}: {e}")
        return 0

def plot_contours_with_probabilities(features_list, image, rf_path, debug=False, save_path=None):
    """
    
    :param features_list: List of dictionaries with individual feature entries (including predicted labels and probabilities).
    :param image: The image on which to draw the contours.
    :param debug: If True, plot the image, otherwise just return the image with contours.
    :param save_path: If provided, save the plot to the specified file path.
    :return: Image with drawn contours, centroids, and legend information.
    """

    max_dim = max(image.shape)
    thick = max(1, int(0.001 * max_dim))
    
    # Define colors for each label
    label_colors = {
        "Aborted": (255, 0, 0),    # Red for Aborted
        "Partially": (0, 0, 255),  # Blue for Partially
        "Normal": (0, 255, 0),     # Green for Normal
    }
    
    # Create a copy of the image to draw on
    image_copy = image.copy()
    #image_copy = cv2.cvtColor(image_copy, cv2.COLOR_BGR2RGB)

    rf_name = os.path.basename(rf_path)
    
    # Iterate over features_list to draw contours
    for feature in features_list:
        contour = feature["Contour"]
        predicted_label = feature["Predicted Label"]
        predicted_probability = feature["Predicted Probability"]
        
        # Compute the centroid of the contour
        M = cv2.moments(contour)
        if M["m00"] != 0:
            cx = int(M["m10"] / M["m00"])
            cy = int(M["m01"] / M["m00"])
        else:
            cx, cy = 0, 0  # If division by zero, place it at (0,0)
            # Draw the contour with a solid or dashed line
        if predicted_probability > 0.7:
            cv2.drawContours(image_copy, [contour], -1, label_colors.get(predicted_label, (255, 255, 255)), thick)
            cv2.drawMarker(image_copy, (cx, cy), (0, 0, 0), markerType=cv2.MARKER_CROSS, markerSize=25, thickness=thick)
        else:
            # Draw a solid contour
            cv2.drawContours(image_copy, [contour], -1, label_colors.get(predicted_label, (255, 255, 255)), thick)

    if True:
        # Create the legend manually
        legend_patches = []
        for label, color in label_colors.items():
            patch = plt.Line2D([0], [0], marker='o', color='w', markerfacecolor=np.array(color)/255, markersize=10, label=f'{label}')
            legend_patches.append(patch)
        
        # Add cross marker to legend
        cross_marker = plt.Line2D([0], [0], marker='+', color='black', linestyle="None", markersize=5, label="High Probability (>70%)")
        legend_patches.append(cross_marker)

        #Add RF info
        rf_info = plt.Line2D([0], [0], linestyle='None', label=f'Random Forest: {rf_name}')
        legend_patches.append(rf_info)
        
        # Display the image with contours and legend
        plt.figure(figsize=(15, 15))
        plt.imshow(image_copy)
        
        # Add the legend
        plt.legend(handles=legend_patches, loc='upper right', fontsize=12, title="Seed Status", title_fontsize=14)

        plt.axis('off')

        if save_path:
            # Save the figure to a file if save_path is provided
            plt.savefig(save_path, bbox_inches='tight', pad_inches=0.1, dpi=300)  
        elif debug:
            plt.show()

    return image_copy
# In[14]:

def predict_labels_with_random_forest(features_list, model):
    """
    Predicts labels and probabilities for features using a trained Random Forest model.

    :param features_list: List of dictionaries with individual feature entries (Area, Perimeter, etc.).
    :param model: Trained Random Forest model.
    :return: Updated features_list with predicted labels and probabilities.
    """
    learned_features = model.feature_names_in_
    
    for feature in features_list:
        feature_vector = [feature[feat] for feat in learned_features]
        
        # Predict the label using the trained Random Forest model
        predicted_label = model.predict([feature_vector])[0]
        feature["Predicted Label"] = predicted_label
        
        # Predict the probabilities for all classes
        predicted_proba = model.predict_proba([feature_vector])[0]
        
        # Add the probability of the predicted label to the feature dictionary
        # This will be a value between 0 and 1 for the predicted class
        label_index = model.classes_.tolist().index(predicted_label)  # Get the index of the predicted label
        feature["Predicted Probability"] = predicted_proba[label_index]
    
    return features_list


# ## Image Segmentation

# In[15]:

def adjust_brightness_to_white(image, threshold=252, max_iterations=50, brightness_step=10):
    """
    Adjust the brightness of an image until the background is approximately white.

    Parameters:
        image (numpy.ndarray): Input image (BGR format).
        threshold (int): Brightness threshold for the background (e.g., 245).
        max_iterations (int): Maximum number of iterations to prevent an infinite loop.
        brightness_step (int): Incremental step to adjust brightness.
    
    Returns:
        numpy.ndarray: Brightened image.
    """
    def get_background_mask(image, threshold=50):
        """
        Generate a mask for the background by excluding dark areas (e.g., seeds).
        
        Parameters:
            image (numpy.ndarray): Input image (BGR format).
            threshold (int): Threshold to exclude dark regions.
        
        Returns:
            numpy.ndarray: Binary mask of the background.
        """
        gray = cv2.cvtColor(image, cv2.COLOR_RGB2GRAY)
        _, mask = cv2.threshold(gray, 0, 255, cv2.THRESH_BINARY + cv2.THRESH_OTSU)
        return mask
    
    # Copy the image to avoid modifying the original
    bright_image = image.copy()
    total_brightness_increase = 0

    for i in range(max_iterations):
        # Create a mask to isolate the background
        background_mask = get_background_mask(bright_image)
        background_pixels = bright_image[background_mask == 255]
        
        # Compute the mean background color
        mean_background_color = np.mean(background_pixels, axis=0)
        
        # Check if the background is close to white
        if np.all(mean_background_color >= threshold):
            break
        
        # Incrementally increase brightness
        bright_image = cv2.convertScaleAbs(bright_image, alpha=1, beta=brightness_step)
        total_brightness_increase += brightness_step
    else:
        print("Max iterations reached. Background may not be fully white.")
        print(f"Total brightness increase: {total_brightness_increase}")
    
    return bright_image, mean_background_color

def process_region(image, mean_background_color, contours, median_perimeter=None):
    features, mean_background_color = get_features_short(contours, image, mean_background_color)
    seeds, median_peri = extract_seeds(image, features, median_perimeter)
    seeds = extend_features(seeds, image, mean_background_color)
    tolerance = median_perimeter/6 if median_perimeter is not None else median_peri/6 if median_peri is not None else 2
    seeds = remove_duplicate_bounding_boxes_with_tolerance(seeds, tolerance = tolerance)

    return seeds, median_peri
    
def hybrid_segmentation(image, mean_background_color):
    dense_region, sparse_region, dense_mask, shift_of_dense_region = find_dense_region(image)
    resized_image = resize_image(dense_region)
    masks = mask_generator.generate(resized_image)
    cont_sam = contours_sam(dense_region, resized_image, masks, dense_mask, shift_of_dense_region, image)
    #cont_sam = [contour for contour in cont_sam if 100 < cv2.arcLength(contour, True) < 1000]

    max_dim = max(image.shape)
    cont_sam = [contour for contour in cont_sam if 0.01*max_dim < cv2.arcLength(contour, True) < 0.1*max_dim]
    
    sparse_blurred = cv2.GaussianBlur(cv2.cvtColor(sparse_region, cv2.COLOR_RGB2GRAY), (25, 25), 0)
    _, binary_image = cv2.threshold(sparse_blurred, 127, 255, cv2.THRESH_BINARY+cv2.THRESH_OTSU)
    contours_sparse, _ = cv2.findContours(binary_image, cv2.RETR_CCOMP, cv2.CHAIN_APPROX_SIMPLE)

    seed_features, median_peri_1 = process_region(image, mean_background_color, cont_sam)
    seed_features2, _ = process_region(image, mean_background_color, contours_sparse, median_perimeter=median_peri_1)

    seed_features.extend(seed_features2)

    return seed_features


# # Final Seed Prediction for an image folder


# In[17]:

def resize_image_oom(image, max_side_length=5000):
    """
    resizes the image if its largest size is > max_side_length
    """
    h,w = image.shape[:2]
    max_side = max(h,w)

    if max_side > max_side_length:
        scaling = max_side_length/max_side
        new_h = int(scaling*h)
        new_w = int(scaling*w)
        resized_image = cv2.resize(image, (new_w, new_h), interpolation=cv2.INTER_AREA)
        print(f"Image resized to {new_w,new_h}")
        return resized_image
    else:
        print(f"Failed to resize image, not bigger than max side length {max_side_length}")
        return image

def regenerate_summary_from_parameters(csv_path, selected_features):

    df = pd.read_csv(csv_path)
    summary = []

    for image_name, group in df.groupby("Image Name"):
        row = {
            "Image Name": image_name,
            "Total": len(group),
            "Normal": (group["Predicted Label"] == "Normal").sum(),
            "Partially": (group["Predicted Label"] == "Partially").sum(),
            "Aborted": (group["Predicted Label"] == "Aborted").sum(),
        }
        total = row["Total"]
        row["%Normal"] = round(row["Normal"] / total * 100, 2)
        row["%Partially"] = round(row["Partially"] / total * 100, 2)
        row["%Aborted"] = round(row["Aborted"] / total * 100, 2)

        if selected_features:
            for feat in selected_features:
                if feat in group.columns:
                    row[f"Mean {feat}"] = round(group[feat].mean(), 2)
                else:
                    print(f"Feature '{feat}' not found.")

        summary.append(row)

    summary_df = pd.DataFrame(summary)
    summary_path = csv_path.replace("parameters", "summary_regenerated").replace(".csv", ".xlsx")
    summary_df.to_excel(summary_path, index=False)

    # Add metadata
    try:
        wb = load_workbook(summary_path)
        ws = wb.create_sheet(title="Metadata_Regenerated", index=0)
        metadata = {
            "Source CSV": csv_path,
            "Run Time": datetime.now().strftime("%Y-%m-%d %H:%M:%S"),
            "Selected Features": ", ".join(selected_features) if selected_features else "All numeric"
        }
        for k, v in metadata.items():
            ws.append([k, str(v)])
        wb.save(summary_path)
    except Exception as e:
        print(f"Metadata write failed: {e}")

    print(f"\nRegenerated summary saved to: {summary_path}")

def prompt_feature_selection(sample_csv_path=None):
    # Load a small sample CSV to extract feature names
    if sample_csv_path:
        df = pd.read_csv(sample_csv_path)
    else:
        # If sample doesn't exist, prompt user to skip selection
        print("No data available for feature selection. \Summary will be generated without selected features. \n Retry after analysis with --regenerate-summary flag.")
        return None

    all_features = df.select_dtypes(include='number').columns.tolist()

    if not all_features:
        print("No numeric features found.")
        return None

    print("\nAvailable Features for Summary:\n")
    for i, feat in enumerate(all_features, start=1):
        print(f"  {i}. {feat}")

    choices = input("\nEnter numbers of features to include (comma-separated), or press Enter to skip: ").strip()

    if not choices:
        selected = None
        return selected

    try:
        selected_indexes = [int(x) - 1 for x in choices.split(",") if x.strip().isdigit()]
        selected = [all_features[i] for i in selected_indexes if 0 <= i < len(all_features)]
    except Exception as e:
        print(f"Invalid input. Error: {e}")
        return all_features

    print("\nSelected Features:")
    print(", ".join(selected))
    confirm = input("Proceed with these? (y/n): ").strip().lower()
    if confirm != 'y':
        print("Process aborted by user.")
        sys.exit(0)

    return selected
    
def classify_seeds_in_folder(input_folder, output_folder, rf_model_path, segmentation_only=False):
    start = time.time()
    input_folder_name = os.path.basename(input_folder)
    print(f"Input folder: {input_folder_name}")

    os.makedirs(output_folder, exist_ok=True)
    output_images_folder = os.path.join(output_folder, 'predicted_images')
    if not segmentation_only:
        os.makedirs(output_images_folder, exist_ok=True)

    all_parameters = []
    summary = []
    
    if not segmentation_only:
        model = joblib.load(rf_model_path)

    image_paths = glob(os.path.join(input_folder, "*.*"))
    num_images = len(image_paths)
    
    if num_images == 0:
        print("No images found in the folder.")
        return

    print(f"Processing {num_images} images...\n")
    
    # Track estimated time per image
    estimated_time_per_image = None 
    progress_bar = tqdm(image_paths, desc="Processing images", bar_format='{l_bar}{bar:40}{r_bar}', unit="image")

    for i, image_path in enumerate(progress_bar):
        image_name = os.path.basename(image_path).split('.')
        image_name = f"{image_name[0]}.{image_name[1]}" if len(image_name) > 2 else image_name[0]

        start_image = time.time()
        
        try:
            image = cv2.imread(image_path)
            if image is None:
                print(f"Failed to load image: {image_path}")
                continue

            h,w = image.shape[:2]
            max_side = max(h,w)
            if max_side>9000: #reduce images with high size
                scaling = 9000/max_side
                new_h = int(scaling*h)
                new_w = int(scaling*w)
                image = cv2.resize(image, (new_w, new_h), interpolation=cv2.INTER_AREA)
                #print(f"Image resized to {new_w,new_h}")
                
            image = cv2.cvtColor(image, cv2.COLOR_BGR2RGB)
            image, mean_background_color = adjust_brightness_to_white(image)
            
            seed_features = hybrid_segmentation(image, mean_background_color)  
            if not seed_features:
                print(f"No seeds detected in {image_name}.")
                summary.append({
                    "Image Name": image_name,
                    "Total": "-",
                    "Normal": "-",
                    "Partially": "-",
                    "Aborted": "-",
                    "%Normal": "-",
                    "%Partially": "-",
                    "%Aborted": "-",
                    "%High Confidence Prediction(>0.7)": "-",
                    "Avg. Prediction Confidence (%)": "-",
                    "Execution Time (s)": "-"
                })
                continue

            if segmentation_only:
                features_df = pd.DataFrame(seed_features)
                features_df.insert(0, "Image Name", image_name)
                all_parameters.append(features_df)

                try:
                    all_parameters_df = pd.concat(all_parameters, ignore_index=True)
                    all_parameters_df.sort_values(by="Image Name", ascending=True, inplace=True)
                    all_parameters_path = os.path.join(output_folder, f"seed_parameters_{input_folder_name}.csv")
                    all_parameters_df.to_csv(all_parameters_path, index=False)
                    print(f"\nParameters saved to {all_parameters_path}.")
                except Exception as e:
                    print(f"Error saving parameters to CSV: {e}")
                    print("Error details:", traceback.format_exc())
                
            else:
                seed_features = predict_labels_with_random_forest(seed_features, model)
                features_df = pd.DataFrame(seed_features)
    
                # Save individual image parameters
                features_df.insert(0, "Image Name", image_name)
                all_parameters.append(features_df)

                try:
                    all_parameters_df = pd.concat(all_parameters, ignore_index=True)
                    all_parameters_df.sort_values(by="Image Name", ascending=True, inplace=True)
                    all_parameters_path = os.path.join(output_folder, f"seed_parameters_{input_folder_name}.csv")
                    all_parameters_df.to_csv(all_parameters_path, index=False)
                    print(f"\nParameters saved to {all_parameters_path}.")
                except Exception as e:
                    print(f"Error saving parameters to CSV: {e}")
                    print("Error details:", traceback.format_exc())
    
                # Summarize seed counts
                label_counts = features_df["Predicted Label"].value_counts()
                total = len(features_df)
                normal = label_counts.get("Normal", 0) / total * 100
                partially = label_counts.get("Partially", 0) / total * 100
                aborted = label_counts.get("Aborted", 0) / total * 100
                confidence = len(features_df[features_df['Predicted Probability']>0.7]) / total * 100
                avg_conf = features_df['Predicted Probability'].mean() * 100
                execution = time.time() - start_image 
                row = {
                    "Image Name": image_name,
                    "Total": len(features_df),
                    "Normal": label_counts.get("Normal", 0),
                    "Partially": label_counts.get("Partially", 0),
                    "Aborted": label_counts.get("Aborted", 0),
                    "%Normal": round(normal, 2),
                    "%Partially": round(partially, 2),
                    "%Aborted": round(aborted, 2),
                    "%High Confidence Prediction(>0.7)": round(confidence,2),
                    "Avg. Prediction Confidence (%)": round(avg_conf,2),
                    "Execution Time (s)": round(execution,2)
                }
                # Add selected numerical features
                selected_features = prompt_feature_selection(sample_csv_path=all_parameters_path)
                if selected_features:
                    for feat in selected_features:
                        if feat in features_df.columns:
                            row[f"Mean {feat}"] = round(features_df[feat].mean(), 2)
                        else:
                            print(f"Warning: Feature '{feat}' not found in DataFrame.")

                summary.append(row)
                
                # Save the image with segmentation and prediction contours
                output_image = plot_contours_with_probabilities(
                    seed_features, image, rf_path=rf_model_path,
                    save_path=f"{output_images_folder}/{image_name}_predicted.jpg"
                )
    
                # Update estimated time per image
                if estimated_time_per_image is None:
                    estimated_time_per_image = execution
                else:
                    estimated_time_per_image = (estimated_time_per_image * i + execution) / (i + 1)
    
                # Estimate remaining time
                remaining_time = estimated_time_per_image * (num_images - (i + 1))
                progress_bar.set_postfix(remaining=f"\033[1;31m {remaining_time:.2f}s\033[0m")
        
        except Exception as e:
            print(f"Error processing image {image_name}: {e}")
            print("Error details:", traceback.format_exc())
            continue  
    
    if not segmentation_only:
        try:
            summary_df = pd.DataFrame(summary)
            summary_df.sort_values(by="Image Name", ascending=True, inplace=True)
            total_execution_time = time.time() - start
            summary_df["Total Time"] = round(total_execution_time, 2)
            summary_path = os.path.join(output_folder, f"seed_summary_{input_folder_name}.xlsx")
            summary_df.to_excel(summary_path, index=False)
            print(f"Summary saved to {summary_path}.")

            # Save summary with metadata
            summary_path = os.path.join(output_folder, f"seed_summary_{input_folder_name}.xlsx")
            summary_df.to_excel(summary_path, index=False)
            
            # Add metadata
            metadata = {
                "Random Forest Model": rf_model_path,
                "Run Time": datetime.now().strftime("%Y-%m-%d %H:%M:%S"),
                "Segmentation Only": segmentation_only,
                "Selected Features": ", ".join(selected_features) if selected_features else None,
                "Total Images Processed": num_images
            }
            
            try:
                wb = load_workbook(summary_path)
                ws = wb.create_sheet(title="Metadata", index=0)
                for key, value in metadata.items():
                    ws.append([key, str(value)])
                wb.save(summary_path)
                print(f"Summary with metadata saved to {summary_path}.")
            except Exception as e:
                print(f"Failed to write metadata: {e}")
        except Exception as e:
            print(f"Error saving summary to Excel: {e}")
            print("Error details:", traceback.format_exc())

    total_execution_time = time.time() - start
    print(f"\nTotal execution time for {input_folder}: {total_execution_time:.2f}s")

def main():
    if not os.getenv("STY"):  # "STY" is only set inside a screen session
        print(f"Not in a screen session. Please run this inside a screen session.")
        sys.exit(0)
    
    parser = argparse.ArgumentParser(description="Process seed images and perform segmentation and/or classification.")
    parser.add_argument("directory", help="Directory containing images to process.")
    parser.add_argument("--rf_model_path", default = '~/TripBlockDefault_RF.pkl', help="Path to the Random Forest model.")
    parser.add_argument("--segmentation-only", action="store_true", 
                        help="If set, only segmentation will be performed without classification or summary.")
    parser.add_argument("--regenerate-summary", action="store_true", help="Regenerate summary from existing CSV.")
    parser.add_argument("--features", type=str, help="Comma-separated list of feature names to include in summary.")
    
    args = parser.parse_args()
    
    selected_features = [f.strip() for f in args.features.split(',')] if args.features else None
    rf_model_path = os.path.expanduser(args.rf_model_path)

    param_csv_path = None

    if args.regenerate_summary:
        param_csv = glob(os.path.join(args.directory, "out/seed_parameters_*.csv"))
        if not param_csv:
            raise FileNotFoundError("No seed_parameters CSV found in the 'out' directory.")
        param_csv_path = param_csv[0]
        selected_features = prompt_feature_selection(param_csv_path)
        regenerate_summary_from_parameters(param_csv_path, selected_features)
        sys.exit(0)
        
    if not os.path.isdir(args.directory):
        raise ValueError(f"Directory '{args.directory}' does not exist!")
    output_folder = os.path.join(args.directory, "out")
        
    print(f"Processing directory: {args.directory}")
    print(f"with random forest model: {rf_model_path}")
    if args.segmentation_only:
        print("In segmentation mode only (no classification or summary)")
        
    confirm = input("\nProceed with these data? (y/n): ")
    if confirm.lower() != 'y':
        print("Process aborted.")
        exit()

    os.makedirs(output_folder, exist_ok=True)

    global mask_generator
    mask_generator = start_env()
    torch.cuda.empty_cache()
    
    classify_seeds_in_folder(input_folder=args.directory, output_folder=output_folder, 
                             rf_model_path=rf_model_path, segmentation_only=args.segmentation_only)

    message = (
        "\n\033[1;32mImage processing completed! Results are saved in the 'out' folder.\n\033[0m "
    )

    print(message) 
    
if __name__ == "__main__":
    main()

